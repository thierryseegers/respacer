/*
	(C) Copyright Thierry Seegers 2015. Distributed under the following license:
 
	Boost Software License - Version 1.0 - August 17th, 2003
 
	Permission is hereby granted, free of charge, to any person or organization
	obtaining a copy of the software and accompanying documentation covered by
	this license (the "Software") to use, reproduce, display, distribute,
	execute, and transmit the Software, and to prepare derivative works of the
	Software, and to permit third-parties to whom the Software is furnished to
	do so, all subject to the following:
 
	The copyright notices in the Software and this entire statement, including
	the above license grant, this restriction and the following disclaimer,
	must be included in all copies of the Software, in whole or in part, and
	all derivative works of the Software, unless such copies or derivative
	works are solely in the form of machine-executable object code generated by
	a source language processor.
 
	THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
	IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
	FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
	SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
	FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
	ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
	DEALINGS IN THE SOFTWARE.
 */

#pragma once

#include <lm/model.hh>

#include <algorithm>
#include <cctype>
#include <fstream>
#include <limits>
#include <map>
#include <string>
#include <utility>
#include <vector>

namespace detail
{
    // Minimal trie.
    // We use it as a special spell-checker that can indicate whether a string is a valid word, an incomplete valid word or neither.
    template<typename T>
    class trie
    {
    public:
        trie<T>* insert(T const& t)
        {
            return &children_[t];
        }
        
        template<typename I>
        trie<T>* insert(I begin, I end)
        {
            trie<T> *p = this;
            
            while(begin != end)
            {
                p = p->insert(*begin);
                ++begin;
            }
            
            return p;
        }
        
        trie<T>* find(T const& t)
        {
            auto i = children_.find(t);
            
            return i == children_.end() ? nullptr : &i->second;
        }
        
        size_t count(T const& t) const
        {
            return children_.count(t);
        }

    private:
        std::map<T, trie<T>> children_;
    };
    
}

//! For a given string of letters and no spaces, returns the likeliest sequence of words.
//!
//! For example, given "theappleisredandthebananaismushy", the likeliest sequence expected is "the apple is red and the banana is mushy".
//!
//! Works by recursively using the given dictionary to find valid words at the head of the given string and then
//! using the given language model to find the best scoring sequence of words.
class respacer
{
public:
    //! \param dictionary_path  Path to a file containing list of words in alphabetical order, one word per line.
    //! \param language_model_path  Path to a language model file for \c lm::ngram::Model to use.
    respacer(std::string const& dictionary_path, std::string const& language_model_path) : model_(language_model_path.c_str())
    {
        // Populate our dictionary trie.
        // We'll insert a '\0' to indicate valid words. e.g.:
        // b->\0
        //  ->e->\0
        //     ->a
        //       ->n->\0
        // That way, we know that "be" is a valid word and that "bea" leads to a valid word.
        std::ifstream dictionary_stream{dictionary_path};
        
        std::string word;
        while(std::getline(dictionary_stream, word))
        {
            std::transform(word.begin(), word.end(), word.begin(), ::tolower);
            dictionary_.insert(word.begin(), word.end())->insert('\0');
        }
    }
    
    //! Given a string of characters with no spaces, computes the most likely original sentence.
    //!
    //! Invokes companion function. This way, we have a clean signature.
    //!
    //! \param letters A string of characters containing alphabetical characters exclusively.
    //! \return A word sequence.
    std::vector<std::string> respace(std::string const& letters)
    {
        std::pair<double, std::vector<std::string>> best(std::numeric_limits<double>::lowest(), {});
        
        respace(letters, best);
        
        return best.second;
    }
    
private:
    //! Recursively analyzes a string of characters for valid words and keeps score of the most likely sequence.
    //!
    //! For a given string, finds all valid words at its head, scores them using lm::ngram::Model and any previous state.
    //! If the intermediary score is better than the current best score, calls itself with the remaining letters.
    //! Stops when there are no more letters to analyze.
    //!
    //! \param remaining_letters The letters to be analyzed.
    //! \param best The word sequence with the highest score.
    //! \param data The stack of current score, word sequence and \c lm::ngram::State.
    void respace(std::string const& remaining_letters, std::pair<double, std::vector<std::string>>& best, std::vector<std::tuple<double, std::string, lm::ngram::State>> const& data = {})
    {
        if(remaining_letters.empty())
        {
            // No more letters, we have a sequence that has outscored the current best.
            // Additionally score the end-of-sentence tag and keep the results if it still outscores the current best.
            
            lm::ngram::State out_state;
            double score = std::get<0>(data.back()) + model_.Score(std::get<2>(data.back()), model_.GetVocabulary().Index("</s>"), out_state);
            
            if(score > best.first)
            {
                std::vector<std::string> words;
                std::transform(data.begin(), data.end(), back_inserter(words), [](auto const& t){ return std::get<1>(t); });
                
                best = {score, words};
            }
        }
        else
        {
            // We still have letters left to build a sentence, analyze them.
 
            // From the remaining letters string, finds all valid words at its head (e.g. "beefzz" -> {"b", "be", "bee", "beef"}
            //
            // Two important efficiency gains:
            //  1. Using a \ref trie<char> to find all valid words. (I tried a sorted vector and it did not perform as well.)
            //  2. Returns lengths from longest to shortest because sentences with fewer words tend to outscore sentences with more words.
            std::vector<std::string::size_type> head_lengths;
            
            {
                detail::trie<char> *p = &dictionary_;
                
                for(std::string::size_type i = 0; p && i != remaining_letters.size(); ++i)
                {
                    p = p->find(remaining_letters[i]);
                    
                    if(p && p->count('\0'))
                    {
                        head_lengths.push_back(i + 1);
                    }
                }
                
                std::reverse(head_lengths.begin(), head_lengths.end());
            }
            
            // For all valid words found at the head of the remaining letters, score the word taking the current model state into account.
            // If this intermediary score is better than the current best score, analyze the leftover letters.
            // Otherwise, stop analyzing this path, it can't get better.
            for(auto const word_length : head_lengths)
            {
                lm::ngram::State const& in_state = data.size() ? std::get<2>(data.back()) : model_.BeginSentenceState();
                lm::ngram::State out_state;

                std::string const word = remaining_letters.substr(0, word_length);
                
                double score = data.size() ? std::get<0>(data.back()) : 0.;
                score += model_.Score(in_state, model_.GetVocabulary().Index(word), out_state);
                
                if(score > best.first)
                {
                    auto d = data;
                    d.emplace_back(score, word, out_state);
                    
                    respace(remaining_letters.substr(word_length), best, d);
                }
            }
        }
    }
    
    lm::ngram::Model model_;
    detail::trie<char> dictionary_;
};

/*!
 \file respacer.h
 
 \brief The only file you need.
 
 \mainpage notitle
 
 \tableofcontents
 
 \section introduction Introduction
 
 \ref respacer is part of a larger project of mine that requires reconstructing sentences that have lost their whitespace.
 For example, given an input of "itisiyourking", we would like to produce "it is i your king", the most likely sentence originally.
 This mini-project serves to capture the work done for that purpose.
 
 To achieve this goal, I have drawn from recent experience working with natural language processing tools.
 
 I dare claim that it's performance is up there but I have little to compare it against.
 It is fast enough in the context of the aforementioned larger project so I will most probably not spend more time to optimize.
 Note also that I've only ever run this project on MacOS but the code is cross-platform and so are its dependencies.
 
 \section technical Technical considerations
 
 \subsection dependencies Dependencies
 
 \ref respacer is dependent on <a href="http://kheafield.com/code/kenlm/">libkenlm</a>.
 \c libkenlm provides the language model analysis facilities.
 \c libkenlm is itself dependent on \c libboost-system, \c libboost-thread, \c libz and \c libbz2.
 
 \subsection input Run-time inputs
 
 In order to use \ref respacer, one must supply two files at run-time:
 - A plain text sorted dictionary file.
   Given that \ref respacer does direct string matching for spell-checking, that file should contain all of a word possible suffixes.
   e.g. "fruit", "fruits" and "fruity".
 - A language model file.
   This file should follow the <a href="http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html">ARPA backoff N-gram models</a> format.
   \ref respacer makes use of the <a href="http://kheafield.com/code/kenlm/">kenlm</a> language model library and so, one can supply a binary-compressed file.
 
 \section thanks Thanks
 
 - Ken Heafield, author of the <a href="http://kheafield.com/code/kenlm/">kenlm</a> language model tools.
 
 \section sample Sample code
 
 This sample code produces an executable that reads a string from standard input and produces a sentence including spaces on the standard output.
 It uses two included files:
  - A dictionary file generated with <a href="http://aspell.net/">aspell</a> (<tt>aspell -d en dump master | aspell -l en expand > aspell_en_expanded</tt>).
  - A compressed language model file generated from <a href="https://www.gutenberg.org/ebooks/1112.txt.utf-8">Shakespeare's Romeo and Juliet</a> using <a href="http://kheafield.com/code/kenlm/">kenlm</a> tools (<tt>bin/lmplz -o 3 < pg1112.txt > romeo_and_juliet.arpa; bin/build_binary romeo_and_juliet.arpa romeo_and_juliet.mmap</tt>).
 
 \include examples/example.cpp
 
 \section license License
 
 \verbinclude LICENSE
 
 */
